# Big Data Overview

Big data is just large amounts of data, amounts so large we had to create and design new tools to handle all of it.

The dividing line for "Big Data" is more about if the tools being used fall under the Big data umbrella, and not a specific amount

Why is big data a thing?

The amount of data that is stored in computer systems has historically grown at least exponentially.


Couple this with storage becoming cheaper and cheaper, more and more data can be stored and then that data can be worked with. 

The tools for working with that data have traditionally been limited in scope and so new generations of tools have to be invented to deal with problems caused by unforeseenly large data sets.

The history of our data tools went as follows:
- Data was originally stored in what were essentially spreadsheets
- Those became too wieldly so we turned to relational databases
- Relational databases are now being slightly phased out for unstructured NOSQL databases
- And now things are moving to distributed systems like Hadoop

# Hadoop Overview

Hadoop is an ecosystem or collection of tools for distributed data management. These tools are often used when dealing with big data.

Hadoop solves the problems of big data by spreading the data and the processing of that data across multiple machines.

The distributive nature of Hadoop means that it can achieve the equivalent processing power of a modern super-computer (or beyond) at radically lower costs using just commodity machines; these machines can even be virtual machines on a cloud like EC2s.

Hadoop is made up of several major modules:
- Hadoop Common: the base libraries and utilities that all Hadoop modules are dependent on.
- Hadoop YARN: Platform for managing computing resources
- Hadoop MapReduce: used for distributive data processing
- Hadoop Distributed File System (HDFS): file system for storing information in Hadoop

There are many software offerings that use Hadoop under the hood to achieve some broad functionality like Apache Hive (data warehouse) or Apache Sqoop (data transfer between Hadoop and relational databases).

Hadoop is primarily written in Java which allows it to take advantage of Java’s WORA capabilities.

# HDFS as Data Source

This is Hadoops File System; this is like a file system on a single machine, separated into folders that contain files, but is spread out and duplicated among many machines.

Because of the distribution, it is massively scalable; proven up to 100pb of data.

HDFS is fault tolerant. It assumes that outages happen normally in the running of a system, not that they are anomalies.

Hadoop will create duplicates of data to ensure that even if one machine goes down the data is still accessible.

HDFS is self-healing. If it notices that a node (a group of data blocks) is down for a significant amount of time, then the data blocks that were lost are reduplicated across the system.

## HDFS Structure

Hadoop runs in a cluster of machines made up of nodes

Name Node: holds all the metadata for the file structure and keeps track of where all the data nodes are. This also controls the processes for replicating data blocks. This will always be supported by a secondary name node.

Data Nodes: Stores the data in data blocks. A node will have many blocks, those blocks are organized into “racks” and will be on a single machine. By default data replication has a two duplicates of a block on one rack and a third duplicate on another machine.

There are, external process that will run to communicate between HDFS and the other parts of Hadoop like MapReduce.

## Hadoop MapReduce:

This is the engine that will actually do the computing on the distributed file system.

It is primarily composed of two parts:
- Job Tracker which orchestrates and schedules tasks.
- Task Trackers which are assigned processes to run by the Job Tracker to actually perform the tasks. They will be created as close to the data nodes that they are manipulating as possible. A job will generally be broken up and distributed across many Task Trackers.

Ultimately this structure allows processing to be done in parallel and therefore to be done quickly and efficiently on the system.

YARN in Hadoop(v2) will actually be tasked with the provisioning of resources to MapReduce for tasks.